Floating point 使用一个尾数和指数来表示真实的数据，指数和尾数都是变化的。
指数允许表示一个大范围的数字，尾数用来保证准确性。这样十进制的点是“浮动的”，
可以在相对于数字的任意位置出现。

如果我们将指数部分替换为一个固定的缩放因子，我们可以使用数字来表示相对于这个常数的值
这个小数点的位置通过这个缩放因子变为固定的。回到number line例子里，缩放因子的值决定了
决定最小的两跟线之间的距离，线的数量是由我们使用多少位来表示这个数字来决定的。我们
可以使用这些来折中数值范围和精度。任何不是这个常数的整数值的部分会四舍五入到最近的点。

跟浮动点不同的是，固定点表示没有一个通用的标准，而是跟领域相关。我们的量化schema
（真实值和量化值之间的对应）有如下要求
1. 真实值和量化值应该是线性的关系，否者的话，固定点的计算结果不能直接对应到真实值的计算结果
2. 允许精确的表示0.f. 如果我们量化和反量化任意的真实值，只有256（一般地，2B）个数能返回
一模一样的数字，其他值都有精度的损失。如果我们保证0.f是这256个数之一，那么DNN可以更加
准确的量化。作者宣称这样能提高精度，因为0在DNN中是一个特殊值（比如padding）。另外，
使0映射到比0大或者比0小的值在量化中会引入bias

因此我们的量化模型只是简单的一个shift，scale 真实的number line 变成一个量化的number line。
对于一组给定的真实值，我们需要最大值/最小值，在[rmin,rmax]范围内，映射到[0,2B-1]的范围内。
所有数值在这个范围内都是线性分布的。

从这里开始，除非特殊说明，我们假设量化的变量用unit8表示，另外我们也可以使用int8表示，
只要调整zero-point值就行

这些使用相同的参数进行量化的值得集合，应该是在相同的一个范围内，比如说某一个层的权重
或者一个节点的激活输出。 我们在后面会在Tensorflow fake quantization节点中找到如何去找
时机的量化范围。首先我们先看一下如何使这些量化层在一个网络中使用。
part of translation of https://sahnimanas.github.io/2018/06/24/quantization-in-tf-lite.html

首先我们先看一下一个卷积如何使用floating-point来表示：
1. 0个或者更多的权重，这些常数用float存储
2. 1个或多个输入，存在float中
3. 前向的函数使用浮点运算计算输出，存储在float中
4. 输出tensor，也是float

这些训练好的网络模型的权重是常量，因此我们知道他们的范围，可以事先进行量化。
一个层的输入，也就是上个层的输出，也是使用自己的参数，但是，量化难道不需要事先知道他们
的范围？答案在于一个层的输出通常依赖有一定范围的输入，这些输入只有一点点的outliers。
我们当然向知道具体的范围可以更精确的量化，对于无法确定的输入，仍然可以期待他们在相似的
范围内。幸运的是，我们已经在训练的时候计算了输出。因此我们可以在训练的时候找出平均的输出范围
使用这些作为真正的输出的代理。当真正一个没有见过的输入的时候，如果我们的范围太小，一个outlier
就会被压扁，如果我们的范围太大，那么就会rounded，希望这些outlier数量少

然后剩下的就是如何计算输出。量化计算的版本要求将float转换成int，但是我们的计算
可能会overflow，因此需要将结果存在更大的整形，然后重新量化成8bit输出。这个在全精度的
卷积实现里面并不是一个关注点，因为硬件处理了float计算中的这些事情。另外我们也需要修改
某些层的逻辑，比如Relu 应该跟Quantized(0)比较而不是0.0f

我们甚至在3步骤上可以更聪明一点。TFlite 使用gemmlowp 计算矩阵乘法，使用int32存储unit8矩阵
的乘积，然后我们可以把bias 量化成int32的范围，可以更加准确。最后在32-bit转换成8bit，(4)
将会计算这一层的输出。我们可以指定量化范围在下一个激活层之后，比如Relu。这将会隐含了
激活的计算并且帮助我们在这一层使用全量化范围。

现在所有东西都已经就位了，就剩下准备以及转换卷积网络为量化模型，这就是Tensorflow
里面的Fake quantization nodes
1. Fake quantization 的第一个角色就是使网络因为量化而对精确的缺失更加不明感。
最简单的量化一个神经网络就是full precision训练，然后把权重转换成fix-point。这个方法
对大模型来说没有问题，但是对包含更少冗余权重的小模型来说，精度的下降会影响到准确性。
使用fake quantization，the rounding effect of quantization is simulated in the 
forward pass as it would occur in actual inference。我们将会去微调权重来适应精度的却是。
在训练的时候，所有的数值都是存储成float，后向还是跟之前一样
2. fake quantization记录了激活的范围，之前讨论过
这些节点防止在训练图，激活会改变量化范围的地方。在网络训练的时候，他们收集 这个节点的
moving average范围。

所有的信息在TF Lite TOCO 工具，除了其他的优化，如何转换量化模型以及如何在TFlite的kernels中
执行。
